# Retrospective — Context Engineering Meetup (2025)

## 1. Overview
This session focused on the current state of context engineering, with contributions covering applied workflows, research, and large-scale retrieval systems. The program included a project review, a research paper walkthrough, an implementation challenge, and a collaborative exploration of AskUnali’s retrieval problem.

Participants: ~45  
Format: project review → paper → implementation challenge → startup challenge → group discussion

---

## 2. Contributions

### 2.1 Daniel — LLM as a Data Analysis Buddy
Practical lessons from building structured analysis workflows with LLMs:
- Use **Pydantic models** to enforce strict output schemas.
- Provide the model with **tools**, not raw data, to reduce costs & hallucinations.
- Apply chain-of-thought **internally** to improve accuracy without revealing reasoning.
- Focus on predictable execution rather than generative improvisation.

### 2.2 Alex — pRAG: Inspectable RAG Pipelines
A component-wise inspection framework for building transparent and measurable RAG systems:
- Decompose retrieval into **testable layers**.
- Benchmark each layer independently on **recall, accuracy, and latency**.
- Reduce guesswork when tuning retrieval components.
- Accelerate **synthetic data generation** for evaluation and testing.
- Selected chunks as the **ground truth** to evaluate pipeline behavior.

### 2.3 Gauthier — Hybrid RAG Under Sparse Input
Strategies for large-scale retrieval with minimal user input:
- Combine vector and symbolic retrieval for stability under sparse queries.
- Use weighting strategies to control relevance across heterogeneous sources.
- Address scale, diversity, and sparse-signal constraints in practical search systems.

### 2.4 AskUnali Challenge
The group explored retrieval, ranking strategies, metadata governance, and context construction under high-stakes constraints.

---

## 3. AskUnali Challenge — Ideas and Directions

### 3.1 Proposed Approaches
- **Geographic context** from IP/browser to adjust prevalence priors.
- **Downweighting ultra-niche studies** to reduce noise and improve precision.
- **Orphan drug mappings** (EMA/FDA) as high-quality structured anchors.
- **Database optimizations** such as heap storage for frequently accessed rows.
- **Multi-stage retrieval** (coarse → fine) to stabilize sparse-input scenarios.
- **Graph-RAG / ontology expansion** for structured condition and symptom relationships.

### 3.2 Trade-Offs
- Geographic priors may obscure rare-edge cases.
- Reduced niche study weighting impacts coverage.
- Multi-stage and graph-based methods add latency.
- More complex DB and graph structures increase maintenance.
- Ontology drift requires versioning and ongoing curation.

---

## 4. Menti Results — Aggregated Signal

### 4.1 Where Context Pipelines Are Weakest
- **Evaluation** (dominant)
- Normalization and ingest as secondary areas

### 4.2 Main Blockers in Context Engineering
- No evaluation framework  
- Fragmented data  
- Lack of validation/automation  
- High coupling to model architecture  

### 4.3 Long-Term Failure Modes
- Hallucinations / poor context  
- Trust in autogenerated context  

### 4.4 Current Failures
- Inconsistent semantics  
- No automated validation  
- Context drift  

### 4.5 How Retrieval Is Evaluated Today
- Mostly manual (spot checks)  
- RAGAS  
- Synthetic sets  
- Few continuous pipelines  

### 4.6 Challenge Priorities
- Ranking strategy for high-stakes outputs  
- Context filtering and metadata governance  

### 4.7 Strategies for Sparse Input
- Multi-stage retrieval  
- Learned ranking  
- Graph-based expansion  
- Priors-based scoring  

---

## 5. Observations
- Strong participation across all contributions.  
- High engagement during the challenge discussion.  
- Broad representation of applied, research, and systems perspectives.  
- Evaluation tooling (RAGAS, LLM-as-a-judge, Langfuse, TruLens) was discussed explicitly.
  The shared conclusion: no tool currently provides reliable, transparent, domain-agnostic, component-level evaluation for context pipelines.

---

## 6. Next Steps
- Contribute your notes and thoughts to this folder.  
- Open issues for Module 2 contributions.  
- Collect proposals for talks, demos, and implementation challenges.

---

## 7. Next Meetup
**Date:** January 28, 2026  
**Module 2: Behavior — Explicit, Inspectable Planning**  
Focus: separating reasoning from execution for transparency, control, and auditability.

Contribution channels:
- GitHub issues  
- Talk proposals  
- Implementation challenges  
- PRs  

